{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain with OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install qdrant_client langchain langchain_huggingface langchain_community langchain_qdrant pypdf openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from google.colab import userdata\n",
    "\n",
    "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
    "qdrant_api_key = userdata.get('QDRANT_API_KEY')\n",
    "\n",
    "llm_name = \"gemini-1.5-flash\"\n",
    "qdrant_url = \"https://0d263df7-e830-4b3f-a1b4.........\"\n",
    "embed_model = HuggingFaceEmbeddings(model_name = \"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q/A Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "\n",
    "prompt_str = \"\"\"\n",
    "Answer the user question briefly\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "question_fetcher = itemgetter(\"question\")\n",
    "llm = ChatOpenAI(model_name = llm_name, openai_api_key = openai.api_key, temperature=0)\n",
    "\n",
    "chain = question_fetcher | prompt | llm | StrOutputParser()\n",
    "query = \"tell me about Lahore\"\n",
    "response = chain.invoke({\"question\": query})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q/A Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from langchain_core.documents import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "loaders = PyPDFLoader(\"\")\n",
    "pages = loaders.load()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150)\n",
    "\n",
    "splits = text_splitter.split_documents(pages)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Qdrant Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    splits,\n",
    "    embed_model,\n",
    "    url = qdrant_url,\n",
    "    prefer_grpc = True,\n",
    "    api_key = qdrant_api_key,\n",
    "    collection_name = \"test_collection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"\n",
    "Answer the user question only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "_prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "num_chunks = 3\n",
    "\n",
    "retriever = qdrant.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\": num_chunks})\n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "    model_name = llm_name,\n",
    "    temperature = 0,\n",
    "    openai_api_key = openai.api_key)\n",
    "\n",
    "query_fetcher = itemgetter(\"question\")\n",
    "setup = {\"question\": query_fetcher, \"context\": query_fetcher | retriever | format_docs}\n",
    "_chain = (setup | _prompt | chat_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query = \"What is the Master Degree Course in Computer Science?\"\n",
    "response = chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "response\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"\n",
    "Answer the user question only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "conversation_history: {chat_history}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "question_fetcher = itemgetter(\"question\")\n",
    "history_fetcher = itemgetter(\"chat_history\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name = llm_name,\n",
    "    temperature = 0,\n",
    "    openai_api_key = openai.api_key)\n",
    "\n",
    "setup = {\"question\": query_fetcher, \"context\": query_fetcher | retriever | format_docs}\n",
    "chain = (setup | _prompt | chat_llm)\n",
    "query = \"user question:\" + query\n",
    "response = \"ai_response:\" + response\n",
    "history.append((query, response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
